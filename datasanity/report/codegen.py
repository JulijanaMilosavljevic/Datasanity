from __future__ import annotations
from dataclasses import dataclass


def _indent(lines: str, n: int = 4) -> str:
    pad = " " * n
    return "\n".join(pad + l if l.strip() else l for l in lines.splitlines())


def generate_training_code(model_suggestion: dict) -> str:
    """
    Returns a ready-to-run python snippet.
    Uses task_hint + feature mix to create a sklearn pipeline.
    Optionally includes XGBoost (commented as optional).
    """
    task = (model_suggestion or {}).get("task_hint", "classification")
    mix = (model_suggestion or {}).get("feature_mix", {}) or {}
    n_cat = int(mix.get("n_categorical", 0) or 0)
    has_cat = n_cat > 0

    is_classification = task == "classification"

    metric_block = (
        "from sklearn.metrics import classification_report\n"
        "print(classification_report(y_val, preds))\n"
        if is_classification
        else
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
        "import numpy as np\n"
        "print('MAE:', mean_absolute_error(y_val, preds))\n"
        "print('RMSE:', np.sqrt(mean_squared_error(y_val, preds)))\n"
    )

    split_block = (
        "from sklearn.model_selection import train_test_split\n"
        "X_train, X_val, y_train, y_val = train_test_split(\n"
        "    X, y, test_size=0.2, random_state=42, stratify=y\n"
        ")\n"
        if is_classification
        else
        "from sklearn.model_selection import train_test_split\n"
        "X_train, X_val, y_train, y_val = train_test_split(\n"
        "    X, y, test_size=0.2, random_state=42\n"
        ")\n"
    )

    base_estimator = (
        "from sklearn.linear_model import LogisticRegression\n"
        "estimator = LogisticRegression(max_iter=2000)\n"
        if is_classification
        else
        "from sklearn.linear_model import Ridge\n"
        "estimator = Ridge(alpha=1.0)\n"
    )

    preprocessing = (
        "from sklearn.compose import ColumnTransformer\n"
        "from sklearn.pipeline import Pipeline\n"
        "from sklearn.impute import SimpleImputer\n"
        "from sklearn.preprocessing import OneHotEncoder\n"
        "\n"
        "# Detect feature types\n"
        "num_cols = X.select_dtypes(include=['number', 'bool']).columns\n"
        "cat_cols = X.select_dtypes(exclude=['number', 'bool']).columns\n"
        "\n"
        "numeric_pipe = Pipeline(steps=[\n"
        "    ('imputer', SimpleImputer(strategy='median')),\n"
        "])\n"
        "\n"
        "categorical_pipe = Pipeline(steps=[\n"
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n"
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n"
        "])\n"
        "\n"
        "preprocess = ColumnTransformer(\n"
        "    transformers=[\n"
        "        ('num', numeric_pipe, num_cols),\n"
        "        ('cat', categorical_pipe, cat_cols),\n"
        "    ],\n"
        "    remainder='drop'\n"
        ")\n"
        if has_cat
        else
        "from sklearn.pipeline import Pipeline\n"
        "from sklearn.impute import SimpleImputer\n"
        "\n"
        "pipeline_pre = Pipeline(steps=[\n"
        "    ('imputer', SimpleImputer(strategy='median')),\n"
        "])\n"
    )

    pipeline_build = (
        "model = Pipeline(steps=[\n"
        "    ('preprocess', preprocess),\n"
        "    ('estimator', estimator)\n"
        "])\n"
        if has_cat
        else
        "from sklearn.pipeline import Pipeline\n"
        "model = Pipeline(steps=[\n"
        "    ('preprocess', pipeline_pre),\n"
        "    ('estimator', estimator)\n"
        "])\n"
    )

    xgb_optional = (
        "\n"
        "# Optional: XGBoost (strong tabular baseline)\n"
        "# pip install xgboost\n"
        "# from xgboost import XGBClassifier, XGBRegressor\n"
        "# if you use XGBoost, keep the same preprocess pipeline\n"
        "# xgb = XGBClassifier(\n"
        "#     n_estimators=400, learning_rate=0.05, max_depth=6,\n"
        "#     subsample=0.9, colsample_bytree=0.9, eval_metric='logloss'\n"
        "# )\n"
        "# model = Pipeline(steps=[('preprocess', preprocess), ('estimator', xgb)])\n"
        if has_cat
        else
        "\n"
        "# Optional: XGBoost (strong tabular baseline)\n"
        "# pip install xgboost\n"
        "# from xgboost import XGBClassifier, XGBRegressor\n"
        "# xgb = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6)\n"
        "# model = Pipeline(steps=[('preprocess', pipeline_pre), ('estimator', xgb)])\n"
    )

    snippet = f"""# Auto-generated by DataSanity
import pandas as pd

# 1) Load your data
df = pd.read_csv("your_data.csv")

target = "TARGET_COLUMN"  # <- set this
# Drop obvious ID-like columns (optional)
# df = df.drop(columns=["id", "customer_id"], errors="ignore")

X = df.drop(columns=[target])
y = df[target]

# 2) Split
{split_block}

# 3) Preprocess + model
{preprocessing}

{base_estimator}

{pipeline_build}

# 4) Train
model.fit(X_train, y_train)

# 5) Evaluate
preds = model.predict(X_val)
{metric_block}
{xgb_optional}
"""
    return snippet
